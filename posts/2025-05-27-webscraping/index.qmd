---
title: "Webscraping"
author: "Harshini Karthikeyan"
date: '05-27-2025'
format:
  html:
    backgroundcolor: whitesmoke
    monobackgroundcolor: lightsteelblue
    fontcolor: black
    number_sections: yes
#mainfont: default
editor: visual
toc: true
toc_float: true
embed-resources: true
image: dashboard_preview.JPG
---

# Web-scraping 

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤

```{r}
library(purrr)
library(tidyverse)
library(stringr)
library(rvest)
```

## **Part 1:** Locate and examine the `robots.txt` file for this website. Summarize

what you learn from it.

<https://www.cheese.com/robots.txt> User-agent: \* Sitemap: <https://www.cheese.com/sitemap.xml>

User-agent: \*: anyone is allowed to scrape No Crawl-delay: no wait time is required between each page scraped No Visit-time entry: no restrictions on time that scraping is allowed No Request-rate entry: no restrictions on simultaneous requests No mention of Disallow sections

## **Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how

this function works with a small example.

It gets the an attribute of a single html element. You can pull various details of an element, such as the reference link, the class, the name, the css styling etc. Below I have initialized a basic html. We stated that the reference link for the first element is a.com and that the class is 'vital'. For the second we do the same but with href being alisa.com and the class being '541'. For the last we can see when the attribute is outside of the <a></a> it is not recognized as an attribute. Thus the class, "active" is not pulled, but the href, harshini.com is pulled.

```{r}

# below we initialize a basic html
html <- minimal_html('<ul>
  <li><a href="https://cheese.com" class="vital">a</a></li>
  <li><a href="https://alisa.com" class="541" >b</a></li>
  <li>class="active"<a href="https://harshini.com"b</a></li>
  </ul>')

# here we run the function to pull the attributes we wrote
html %>% html_elements("a") %>% html_attrs()


```

## **Part 3:** (Do this alongside Part 4 below.) I

used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.

Fully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries: already have a libraries code chunk, and reloading the libraries every time, feels inefficient, but not incredibly so. 
library(rvest) 
library(dplyr)


# Define the URL: this seems to be correct and defining this individually makes it easier should we need to change the url or use it somewhere else
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage: 
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  # cheese-item is non-existent, this should be h3 a, as when we ran that we had legitimate output
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)
  #the outcome of this is "https://cheese.com" which is not what we would want: so the paste0 function is useless
  # the general structure is useful though, and when you separate the flow into two it seems to work


cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  # again .cheese-item is non-existent so we would want to remove that, but then it runs fine
  html_text()


# this works well, when the above is fixed
# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)


# Print the data frame
print(cheese_df)
```

**Part 4:** Obtain the following information for **all** cheeses in the database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)

```{r}
get_text_from_page <- function(page, css_selector) {
    
  page |> 
    html_elements(css_selector) |> 
    html_text()
}

scrape_page <- function(url) {
    
    # 1 second pause between page queries
    Sys.sleep(1)
    
    # Read the page
    page <- read_html(url)
    
    # Grab cheese name from the page
    cheese_name <- get_text_from_page(page, "h3")
    
    # Grab link from cheese node within page
    cheese_href <- page |> 
      html_nodes("h3 a") |> 
      html_attr("href")
    
    # make it look like a url
    cheese_url <- paste0("https://cheese.com", cheese_href)
    
    # Grab the main body image elements
    cheese_pic_reference <- page |> 
      html_elements("#main-body img") 
    
    # If the class of the cheese pic image is image-exists, then set cheese_pic_ifelse to true
    cheese_pic_ifelse <- cheese_pic_reference |> 
      html_attr("class") |> 
      str_detect("image-exists") 
    
    #Make a tibble
    tibble(
        name = cheese_name,
        url = cheese_url,
        has_pic = cheese_pic_ifelse
    )
}


```

```{r}
base_url <- "https://www.cheese.com/alphabetical/?per_page=100"

urls_all_pages <- c(base_url, 
                    str_c(base_url, 
                          "&page=", 
                          1:21)
                    )

pages <- map(urls_all_pages, scrape_page)

df_cheeses <- bind_rows(pages)

head(df_cheeses)
```

**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)

```{r}
#Extract a certain amount of cheese links per page
extract_links <- function(main_url, n = 10) {
  # 1 second pause between page queries
  Sys.sleep(1)
  
  #Read the page
  page <- read_html(main_url)
  
  #Get the first n urls for cheeses
  cheese_href <- page |>
    html_nodes("h3 a") |> 
    head(n) |>
    html_attr("href")
  
  #get cheese url
  paste0("https://cheese.com", cheese_href)
}

#Helper function to extract the specific cheese information
extract_field <- function(field_name, cheese_text) {
  # Search for lines starting with field
  field <- cheese_text[str_detect(cheese_text, paste0("^", field_name))]
  
  if (length(field) == 0) return(NA)
  
  trimws(str_remove(field, field_name))
}


#Scrape each individual cheese page
cheese_scrape <- function(cheese_url) {
 #1 second pause between page queries
  Sys.sleep(1)
  
  #Read the page
  page <- read_html(cheese_url)
    
  # Get the cheese stuff
  cheese_info_items <- page |> 
    html_elements("li p") |> 
    html_text()
    
  tibble(
    milk = extract_field("Made from", cheese_info_items),
    country = extract_field("Country of origin:", cheese_info_items),
    family = extract_field("Family:", cheese_info_items),
    type = extract_field("Type:", cheese_info_items),
    flavour = extract_field("Flavour:", cheese_info_items)
  )
}
  
```

```{r}
cheese_urls <- extract_links("https://www.cheese.com/alphabetical/")

pages2 <- map(cheese_urls, cheese_scrape)

df_cheeses2 <- bind_rows(pages2)
```

```{r}
df_cheeses2
```

**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To what extent do your function(s) adhere to the **principles for writing good functions**? To what extent are your **functions efficient**? To what extent is your **iteration of these functions efficient**?

Our code for scraping cheese data from cheese.com is fairly efficient and follows several principles of writing good functions. First of all, our functions are well-named, each with a single clear purpose. Scrape_page() extracts cheese names, URLs, and image existence, while cheese_scrape() focuses on extracting detailed cheese characteristics from individual pages. This separation improved reusability, and allows our function to be cleaner. We did our best to make our functions flexible. For example, extract_links() allows us to set how many cheese URLs to extract. Efficiency-wise, the code uses vectorized operations to the best of our knowledge and avoids repeated work, such as reloading libraries or scraping unnecessary pages. We used map() for iteration over multiple pages ensuring cleaner application of functions across a list of URLs. Additionally, adding the Sys.sleep(1) pause respects the serverâ€™s resources. One area for potential improvement could be if we optimized the scraping logic to avoid redundant parsing, some operations could be done once per page instead od multiple times per element.

Overall, I think we did pretty good, but there are still ways we could make our code more efficient!

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤

```{r}
library(purrr)
library(tidyverse)
library(stringr)
library(rvest)
```

## **Part 1:** Locate and examine the `robots.txt` file for this website. Summarize

what you learn from it.

<https://www.cheese.com/robots.txt> User-agent: \* Sitemap: <https://www.cheese.com/sitemap.xml>

User-agent: \*: anyone is allowed to scrape No Crawl-delay: no wait time is required between each page scraped No Visit-time entry: no restrictions on time that scraping is allowed No Request-rate entry: no restrictions on simultaneous requests No mention of Disallow sections

## **Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how

this function works with a small example.

It gets the an attribute of a single html element. You can pull various details of an element, such as the reference link, the class, the name, the css styling etc. Below I have initialized a basic html. We stated that the reference link for the first element is a.com and that the class is 'vital'. For the second we do the same but with href being alisa.com and the class being '541'. For the last we can see when the attribute is outside of the <a></a> it is not recognized as an attribute. Thus the class, "active" is not pulled, but the href, harshini.com is pulled.

```{r}

# below we initialize a basic html
html <- minimal_html('<ul>
  <li><a href="https://cheese.com" class="vital">a</a></li>
  <li><a href="https://alisa.com" class="541" >b</a></li>
  <li>class="active"<a href="https://harshini.com"b</a></li>
  </ul>')

# here we run the function to pull the attributes we wrote
html %>% html_elements("a") %>% html_attrs()


```

## **Part 3:** (Do this alongside Part 4 below.) I

used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.

Fully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries: already have a libraries code chunk, and reloading the libraries every time, feels inefficient, but not incredibly so. 
library(rvest) 
library(dplyr)


# Define the URL: this seems to be correct and defining this individually makes it easier should we need to change the url or use it somewhere else
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage: 
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  # cheese-item is non-existent, this should be h3 a, as when we ran that we had legitimate output
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)
  #the outcome of this is "https://cheese.com" which is not what we would want: so the paste0 function is useless
  # the general structure is useful though, and when you separate the flow into two it seems to work


cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  # again .cheese-item is non-existent so we would want to remove that, but then it runs fine
  html_text()


# this works well, when the above is fixed
# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)


# Print the data frame
print(cheese_df)
```

**Part 4:** Obtain the following information for **all** cheeses in the database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)

```{r}
get_text_from_page <- function(page, css_selector) {
    
  page |> 
    html_elements(css_selector) |> 
    html_text()
}

scrape_page <- function(url) {
    
    # 1 second pause between page queries
    Sys.sleep(1)
    
    # Read the page
    page <- read_html(url)
    
    # Grab cheese name from the page
    cheese_name <- get_text_from_page(page, "h3")
    
    # Grab link from cheese node within page
    cheese_href <- page |> 
      html_nodes("h3 a") |> 
      html_attr("href")
    
    # make it look like a url
    cheese_url <- paste0("https://cheese.com", cheese_href)
    
    # Grab the main body image elements
    cheese_pic_reference <- page |> 
      html_elements("#main-body img") 
    
    # If the class of the cheese pic image is image-exists, then set cheese_pic_ifelse to true
    cheese_pic_ifelse <- cheese_pic_reference |> 
      html_attr("class") |> 
      str_detect("image-exists") 
    
    #Make a tibble
    tibble(
        name = cheese_name,
        url = cheese_url,
        has_pic = cheese_pic_ifelse
    )
}


```

```{r}
base_url <- "https://www.cheese.com/alphabetical/?per_page=100"

urls_all_pages <- c(base_url, 
                    str_c(base_url, 
                          "&page=", 
                          1:21)
                    )

pages <- map(urls_all_pages, scrape_page)

df_cheeses <- bind_rows(pages)

head(df_cheeses)
```

**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)

```{r}
#Extract a certain amount of cheese links per page
extract_links <- function(main_url, n = 10) {
  # 1 second pause between page queries
  Sys.sleep(1)
  
  #Read the page
  page <- read_html(main_url)
  
  #Get the first n urls for cheeses
  cheese_href <- page |>
    html_nodes("h3 a") |> 
    head(n) |>
    html_attr("href")
  
  #get cheese url
  paste0("https://cheese.com", cheese_href)
}

#Helper function to extract the specific cheese information
extract_field <- function(field_name, cheese_text) {
  # Search for lines starting with field
  field <- cheese_text[str_detect(cheese_text, paste0("^", field_name))]
  
  if (length(field) == 0) return(NA)
  
  trimws(str_remove(field, field_name))
}


#Scrape each individual cheese page
cheese_scrape <- function(cheese_url) {
 #1 second pause between page queries
  Sys.sleep(1)
  
  #Read the page
  page <- read_html(cheese_url)
    
  # Get the cheese stuff
  cheese_info_items <- page |> 
    html_elements("li p") |> 
    html_text()
    
  tibble(
    milk = extract_field("Made from", cheese_info_items),
    country = extract_field("Country of origin:", cheese_info_items),
    family = extract_field("Family:", cheese_info_items),
    type = extract_field("Type:", cheese_info_items),
    flavour = extract_field("Flavour:", cheese_info_items)
  )
}
  
```

```{r}
cheese_urls <- extract_links("https://www.cheese.com/alphabetical/")

pages2 <- map(cheese_urls, cheese_scrape)

df_cheeses2 <- bind_rows(pages2)
```

```{r}
df_cheeses2
```

**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To what extent do your function(s) adhere to the **principles for writing good functions**? To what extent are your **functions efficient**? To what extent is your **iteration of these functions efficient**?

Our code for scraping cheese data from cheese.com is fairly efficient and follows several principles of writing good functions. First of all, our functions are well-named, each with a single clear purpose. Scrape_page() extracts cheese names, URLs, and image existence, while cheese_scrape() focuses on extracting detailed cheese characteristics from individual pages. This separation improved reusability, and allows our function to be cleaner. We did our best to make our functions flexible. For example, extract_links() allows us to set how many cheese URLs to extract. Efficiency-wise, the code uses vectorized operations to the best of our knowledge and avoids repeated work, such as reloading libraries or scraping unnecessary pages. We used map() for iteration over multiple pages ensuring cleaner application of functions across a list of URLs. Additionally, adding the Sys.sleep(1) pause respects the serverâ€™s resources. One area for potential improvement could be if we optimized the scraping logic to avoid redundant parsing, some operations could be done once per page instead od multiple times per element.

Overall, I think we did pretty good, but there are still ways we could make our code more efficient!

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤

```{r}
library(purrr)
library(tidyverse)
library(stringr)
library(rvest)
```

## **Part 1:** Locate and examine the `robots.txt` file for this website. Summarize

what you learn from it.

<https://www.cheese.com/robots.txt> User-agent: \* Sitemap: <https://www.cheese.com/sitemap.xml>

User-agent: \*: anyone is allowed to scrape No Crawl-delay: no wait time is required between each page scraped No Visit-time entry: no restrictions on time that scraping is allowed No Request-rate entry: no restrictions on simultaneous requests No mention of Disallow sections

## **Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how

this function works with a small example.

It gets the an attribute of a single html element. You can pull various details of an element, such as the reference link, the class, the name, the css styling etc. Below I have initialized a basic html. We stated that the reference link for the first element is a.com and that the class is 'vital'. For the second we do the same but with href being alisa.com and the class being '541'. For the last we can see when the attribute is outside of the <a></a> it is not recognized as an attribute. Thus the class, "active" is not pulled, but the href, harshini.com is pulled.

```{r}

# below we initialize a basic html
html <- minimal_html('<ul>
  <li><a href="https://cheese.com" class="vital">a</a></li>
  <li><a href="https://alisa.com" class="541" >b</a></li>
  <li>class="active"<a href="https://harshini.com"b</a></li>
  </ul>')

# here we run the function to pull the attributes we wrote
html %>% html_elements("a") %>% html_attrs()


```

## **Part 3:** (Do this alongside Part 4 below.) I

used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.

Fully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries: already have a libraries code chunk, and reloading the libraries every time, feels inefficient, but not incredibly so. 
library(rvest) 
library(dplyr)


# Define the URL: this seems to be correct and defining this individually makes it easier should we need to change the url or use it somewhere else
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage: 
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  # cheese-item is non-existent, this should be h3 a, as when we ran that we had legitimate output
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)
  #the outcome of this is "https://cheese.com" which is not what we would want: so the paste0 function is useless
  # the general structure is useful though, and when you separate the flow into two it seems to work


cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  # again .cheese-item is non-existent so we would want to remove that, but then it runs fine
  html_text()


# this works well, when the above is fixed
# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)


# Print the data frame
print(cheese_df)
```

**Part 4:** Obtain the following information for **all** cheeses in the database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)

```{r}
get_text_from_page <- function(page, css_selector) {
    
  page |> 
    html_elements(css_selector) |> 
    html_text()
}

scrape_page <- function(url) {
    
    # 1 second pause between page queries
    Sys.sleep(1)
    
    # Read the page
    page <- read_html(url)
    
    # Grab cheese name from the page
    cheese_name <- get_text_from_page(page, "h3")
    
    # Grab link from cheese node within page
    cheese_href <- page |> 
      html_nodes("h3 a") |> 
      html_attr("href")
    
    # make it look like a url
    cheese_url <- paste0("https://cheese.com", cheese_href)
    
    # Grab the main body image elements
    cheese_pic_reference <- page |> 
      html_elements("#main-body img") 
    
    # If the class of the cheese pic image is image-exists, then set cheese_pic_ifelse to true
    cheese_pic_ifelse <- cheese_pic_reference |> 
      html_attr("class") |> 
      str_detect("image-exists") 
    
    #Make a tibble
    tibble(
        name = cheese_name,
        url = cheese_url,
        has_pic = cheese_pic_ifelse
    )
}


```

```{r}
base_url <- "https://www.cheese.com/alphabetical/?per_page=100"

urls_all_pages <- c(base_url, 
                    str_c(base_url, 
                          "&page=", 
                          1:21)
                    )

pages <- map(urls_all_pages, scrape_page)

df_cheeses <- bind_rows(pages)

head(df_cheeses)
```

**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)

```{r}
#Extract a certain amount of cheese links per page
extract_links <- function(main_url, n = 10) {
  # 1 second pause between page queries
  Sys.sleep(1)
  
  #Read the page
  page <- read_html(main_url)
  
  #Get the first n urls for cheeses
  cheese_href <- page |>
    html_nodes("h3 a") |> 
    head(n) |>
    html_attr("href")
  
  #get cheese url
  paste0("https://cheese.com", cheese_href)
}

#Helper function to extract the specific cheese information
extract_field <- function(field_name, cheese_text) {
  # Search for lines starting with field
  field <- cheese_text[str_detect(cheese_text, paste0("^", field_name))]
  
  if (length(field) == 0) return(NA)
  
  trimws(str_remove(field, field_name))
}


#Scrape each individual cheese page
cheese_scrape <- function(cheese_url) {
 #1 second pause between page queries
  Sys.sleep(1)
  
  #Read the page
  page <- read_html(cheese_url)
    
  # Get the cheese stuff
  cheese_info_items <- page |> 
    html_elements("li p") |> 
    html_text()
    
  tibble(
    milk = extract_field("Made from", cheese_info_items),
    country = extract_field("Country of origin:", cheese_info_items),
    family = extract_field("Family:", cheese_info_items),
    type = extract_field("Type:", cheese_info_items),
    flavour = extract_field("Flavour:", cheese_info_items)
  )
}
  
```

```{r}
cheese_urls <- extract_links("https://www.cheese.com/alphabetical/")

pages2 <- map(cheese_urls, cheese_scrape)

df_cheeses2 <- bind_rows(pages2)
```

```{r}
df_cheeses2
```

**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To what extent do your function(s) adhere to the **principles for writing good functions**? To what extent are your **functions efficient**? To what extent is your **iteration of these functions efficient**?

Our code for scraping cheese data from cheese.com is fairly efficient and follows several principles of writing good functions. First of all, our functions are well-named, each with a single clear purpose. Scrape_page() extracts cheese names, URLs, and image existence, while cheese_scrape() focuses on extracting detailed cheese characteristics from individual pages. This separation improved reusability, and allows our function to be cleaner. We did our best to make our functions flexible. For example, extract_links() allows us to set how many cheese URLs to extract. Efficiency-wise, the code uses vectorized operations to the best of our knowledge and avoids repeated work, such as reloading libraries or scraping unnecessary pages. We used map() for iteration over multiple pages ensuring cleaner application of functions across a list of URLs. Additionally, adding the Sys.sleep(1) pause respects the serverâ€™s resources. One area for potential improvement could be if we optimized the scraping logic to avoid redundant parsing, some operations could be done once per page instead od multiple times per element.

Overall, I think we did pretty good, but there are still ways we could make our code more efficient!
